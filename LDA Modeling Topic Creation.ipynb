{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6126034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp36-cp36m-win_amd64.whl (23.9 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\hrg16\\anaconda3\\envs\\bigdataist718\\lib\\site-packages (from gensim) (0.8)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\hrg16\\anaconda3\\envs\\bigdataist718\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp36-cp36m-win_amd64.whl (1.6 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\hrg16\\anaconda3\\envs\\bigdataist718\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.23\n",
      "    Uninstalling Cython-0.29.23:\n",
      "      Successfully uninstalled Cython-0.29.23\n",
      "Successfully installed Cython-0.29.21 gensim-4.0.1 smart-open-5.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fbprophet 0.7.1 requires cmdstanpy==0.9.5, which is not installed.\n",
      "fbprophet 0.7.1 requires setuptools-git>=1.2, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec6f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Author: Hayden Gill\n",
    "##Last Updated: 5/26/2021\n",
    "##Purpose: Perform LDA on Reddit WallStreetBets data, as part of IST736 Final Project\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import temp_script\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import matutils\n",
    "from gensim.test import test_utils, utils\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18f24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"wsb_comments_results.csv\")\n",
    "posts = pd.read_csv(\"wsb_post_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d324bda",
   "metadata": {},
   "source": [
    "Set comments & posts index so as to re-combine later - and select just whats needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0468e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 body\n",
      "id      parent_id                                                    \n",
      "gftdfol t1_gftdcb8                                            dipping\n",
      "gftdfo4 t1_gftdc1s                                   nio and spce ftw\n",
      "gftdfo3 t1_gftdbth                                               flat\n",
      "gftdfnp t3_kcvkwx   lol what happened to the limit up everyones be...\n",
      "gftdfmf t1_gftddng                                               jan \n",
      "...                                                               ...\n",
      "gkdmw91 t1_gkceomw  lmaogot nights to sleep over itmight belittle ...\n",
      "gkdmw6k t3_l3c0hj   squeeze aside are there price targets for gmes...\n",
      "gkdmw1y t1_gkdloa8                                      td ameritrade\n",
      "gkdmvxa t1_gkbyl29  same ive gotin savings but thinking of droppin...\n",
      "gkdmvqi t1_gkdgfwc  imagine being so uneducated and ingnorant you ...\n",
      "\n",
      "[9486843 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "comments = comments.loc[comments['author'] != \"AutoModerator\",:]\n",
    "comments.set_index(['id','parent_id'], inplace = True)\n",
    "comments_min = pd.DataFrame(comments['body'])\n",
    "print(comments_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366c34b",
   "metadata": {},
   "source": [
    "Set posts index so as to re-combine later, and select whats needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80845238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     body\n",
      "id                                                       \n",
      "kcz0hf   am woke up five minutes ago made some instant...\n",
      "kcz07r                                            removed\n",
      "kcyyw4                                            removed\n",
      "kcyyve                                            removed\n",
      "kcyyo1   intro let me get soft with ya it can get ugly...\n",
      "...                                                   ...\n",
      "kl8wbp  i dont know how the fuck you guys became so po...\n",
      "kl8tjy                                            removed\n",
      "kl8m4y  saw the trends and tryingback test to see if h...\n",
      "kl8f6l  large gamestop investor ryan cohen buys more s...\n",
      "kl8f3t   trading plan daily routine mf premarket am lo...\n",
      "\n",
      "[424107 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "posts.set_index(['id'], inplace = True)\n",
    "posts_min  = pd.DataFrame(posts['body'])\n",
    "print(posts_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235523df",
   "metadata": {},
   "source": [
    "Removing the \"removed\" bodies from both comments and posts, as well as any with a word length less than 5. \"lets all invest in gamestop\" is basically short enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "eb1e0a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     body\n",
      "id                                                       \n",
      "kcz0hf   am woke up five minutes ago made some instant...\n",
      "kcz07r                                            removed\n",
      "kcyyw4                                            removed\n",
      "kcyyve                                            removed\n",
      "kcyyo1   intro let me get soft with ya it can get ugly...\n",
      "...                                                   ...\n",
      "kl8wbp  i dont know how the fuck you guys became so po...\n",
      "kl8tjy                                            removed\n",
      "kl8m4y  saw the trends and tryingback test to see if h...\n",
      "kl8f6l  large gamestop investor ryan cohen buys more s...\n",
      "kl8f3t   trading plan daily routine mf premarket am lo...\n",
      "\n",
      "[424107 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "##Dev area for below block - delete when done\n",
    "print(posts_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d313d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducing posts:\n",
      "327625 'removed' dropped\n",
      "43055 shorter-than-5 posts dropped\n",
      "\n",
      "\n",
      "reducing comments:\n",
      "2068380 'removed' dropped\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "temp_posts = posts_min.copy(deep = True)\n",
    "temp_comments = comments_min.copy(deep = True)\n",
    "##handle the posts\n",
    "print(\"reducing posts:\")\n",
    "before_removed = len(temp_posts)\n",
    "temp_posts = temp_posts.loc[temp_posts['body'] != \"removed\",:]\n",
    "after_removed = len(temp_posts)\n",
    "print(str(before_removed - after_removed) + \" 'removed' dropped\")\n",
    "\n",
    "before_length = len(temp_posts)\n",
    "posts_word_counts = temp_posts['body'].apply(lambda x: len(nltk.word_tokenize(str(x))))\n",
    "temp_posts = temp_posts.loc[(posts_word_counts >= 5),:]\n",
    "after_length = len(temp_posts)\n",
    "print(str(before_length - after_length) + \" shorter-than-5 posts dropped\")\n",
    "\n",
    "print(\"\\n\\nreducing comments:\")\n",
    "before_removed = len(temp_comments)\n",
    "temp_comments = temp_comments.loc[temp_comments['body'] != \"removed\",:]\n",
    "after_removed = len(temp_comments)\n",
    "print(str(before_removed - after_removed) + \" 'removed' dropped\") \n",
    "\n",
    "before_length = len(temp_comments)\n",
    "comments_word_counts = temp_comments['body'].apply(lambda x: len(nltk.word_tokenize(str(x))))\n",
    "print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ab77d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2201204 shorter-than-5 comments dropped\n"
     ]
    }
   ],
   "source": [
    "#print(comments_word_counts)\n",
    "#print(len(temp_comments))\n",
    "mask = comments_word_counts >=5\n",
    "temp_comments = temp_comments.loc[mask,:]\n",
    "print(str(len(mask) - len(temp_comments)) + \" shorter-than-5 comments dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5dc817",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-f206a889063b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mcomments_min\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtemp_comments\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mcomments_min\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"comments_rm_short_removed.csv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mposts_min\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtemp_posts\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mposts_min\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"posts_rm_short_removed\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'temp_comments' is not defined"
     ]
    }
   ],
   "source": [
    "comments_min = temp_comments\n",
    "comments_min.to_csv(\"comments_rm_short_removed.csv\")\n",
    "\n",
    "posts_min = temp_posts\n",
    "posts_min.to_csv(\"posts_rm_short_removed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8fd310",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"comments_rm_short_removed.csv\")\n",
    "posts = pd.read_csv(\"posts_rm_short_removed.csv\")\n",
    "comments.set_index(['id','parent_id'], inplace = True)\n",
    "comments_min = pd.DataFrame(comments['body'])\n",
    "\n",
    "posts.set_index(['id'], inplace = True)\n",
    "posts_min  = pd.DataFrame(posts['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8a973",
   "metadata": {},
   "source": [
    "With the short posts and removed posts eliminated, we can format the text with count vectorizer, and think about removing\n",
    "the infrequent words, choices for tokenization, stemming, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b50f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "##function that can be extended later to improve LDA results with combinations of tokenization, frequency, spellchecks, stemming, and lemmatization\n",
    "def count_vectorize(documents, doc_frequency_threshhold = 10, tokenizer = None, stemming_yes_no = False, lemming_yes_no = False, spelling_yes_no = False):\n",
    "    #print(\"Hello\")\n",
    "    cv_transformer = CountVectorizer(input = 'content'\n",
    "                                , lowercase = True\n",
    "                                , tokenizer = simple_preprocess\n",
    "                                , stop_words = 'english'\n",
    "                                , ngram_range = (1,3)\n",
    "                                , min_df = doc_frequency_threshhold\n",
    "                                , max_df = 1.0\n",
    "                                , binary = True)\n",
    "    if stemming_yes_no:\n",
    "        print(\"stem\")\n",
    "        stemmer = PorterStemmer()\n",
    "        documents = pd.DataFrame([' '.join([stemmer.stem(word) for word in text.split(' ')]) for text in documents['body']], columns = ['content'])\n",
    "    \n",
    "    if lemming_yes_no:\n",
    "        print(\"lem\")\n",
    "        lemmer = WordNetLemmatizer()\n",
    "        documents = pd.DataFrame([' '.join([lemmer.lemmatize(word) for word in text.split(' ')]) for text in documents['body']], columns = ['content'])\n",
    "\n",
    "    if spelling_yes_no:\n",
    "        print(\"spell\")\n",
    "    #print(documents)\n",
    "    #print(type(documents))\n",
    "    doc_matrix = cv_transformer.fit_transform(documents['body'])\n",
    "    vocab = cv_transformer.get_feature_names()\n",
    "    print(\"\\nThe produced Term/Document matrix vocab is of size: \" + str(len(vocab)))\n",
    "    #print(\"\\n and includes words: \")\n",
    "    #print(vocab)\n",
    "    print(str((doc_matrix.sum()/(len(vocab)*len(documents)))*100) + \"% Non-Sparse\")\n",
    "    return(doc_matrix, vocab, cv_transformer)\n",
    "\n",
    "def model_lda(cvect_doc_fit, vocabulary, num_topics = 5, passes = 20, modeling_cores = 7):\n",
    "    print(\"Fitting LDA...\")\n",
    "    word_mapping = dict([(i, s) for i, s in enumerate(vocabulary)])\n",
    "    temp = LdaMulticore(matutils.Sparse2Corpus(cvect_doc_fit.T)\n",
    "                        , num_topics = num_topics\n",
    "                        , passes = passes\n",
    "                        , workers = modeling_cores\n",
    "                        , id2word = word_mapping)\n",
    "    return temp\n",
    "\n",
    "def print_topics(lda, vocab, n=10):\n",
    "    topics = lda.show_topics()\n",
    "    #for ti, topic in enumerate(topics):\n",
    "        #print('topic %d: %s' % (ti, ' '.join('%s/%.2f' % (t[1], t[0]) for t in topic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03079c6e",
   "metadata": {},
   "source": [
    "Now perform LDA modeling on Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dad198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The produced Term/Document matrix vocab is of size: 653\n",
      "2.5396219100006534% Non-Sparse\n"
     ]
    }
   ],
   "source": [
    "#posts_corpus_basic, vocabulary = count_vectorize(posts_min, doc_frequency_threshhold = 5) .1%\n",
    "#posts_corpus_basic, vocabulary = count_vectorize(posts_min, doc_frequency_threshhold = 10) .15%\n",
    "#posts_corpus_basic, vocabulary = count_vectorize(posts_min, doc_frequency_threshhold = 20)\n",
    "#posts_corpus_basic, vocabulary = count_vectorize(posts_min, doc_frequency_threshhold = 50)\n",
    "#posts_corpus_basic, vocabulary = count_vectorize(posts_min, doc_frequency_threshhold = 100) #99.18% sparse\n",
    "#posts_corpus_basic, vocabulary = count_vectorize(posts_min, doc_frequency_threshhold = 200) #98.5% sparse\n",
    "posts_corpus_basic, vocabulary, posts_cv = count_vectorize(posts_min, doc_frequency_threshhold = 500)# 96% sparse, acceptable for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd75294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14)\t1\n",
      "  (0, 575)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 582)\t1\n",
      "  (0, 305)\t1\n",
      "  (0, 348)\t1\n",
      "  (0, 470)\t1\n",
      "  (0, 354)\t1\n",
      "  (0, 651)\t1\n",
      "  (0, 533)\t1\n",
      "  (0, 309)\t1\n",
      "  (0, 540)\t1\n",
      "  (0, 618)\t1\n",
      "  (0, 176)\t1\n",
      "  (0, 561)\t1\n",
      "  (0, 188)\t1\n",
      "  (0, 421)\t1\n",
      "  (0, 531)\t1\n",
      "  (0, 109)\t1\n",
      "  (0, 646)\t1\n",
      "  (0, 448)\t1\n",
      "  (0, 487)\t1\n",
      "  (0, 255)\t1\n",
      "  (0, 84)\t1\n",
      "  (0, 214)\t1\n",
      "  :\t:\n",
      "  (0, 370)\t1\n",
      "  (0, 300)\t1\n",
      "  (0, 201)\t1\n",
      "  (0, 413)\t1\n",
      "  (0, 149)\t1\n",
      "  (0, 217)\t1\n",
      "  (0, 594)\t1\n",
      "  (0, 505)\t1\n",
      "  (0, 569)\t1\n",
      "  (0, 353)\t1\n",
      "  (0, 200)\t1\n",
      "  (0, 293)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 77)\t1\n",
      "  (0, 424)\t1\n",
      "  (0, 257)\t1\n",
      "  (0, 524)\t1\n",
      "  (0, 152)\t1\n",
      "  (0, 127)\t1\n",
      "  (0, 372)\t1\n",
      "  (0, 335)\t1\n",
      "  (0, 144)\t1\n",
      "  (0, 131)\t1\n",
      "  (0, 534)\t1\n",
      "  (0, 339)\t1\n"
     ]
    }
   ],
   "source": [
    "print(posts_corpus_basic[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abae4257",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp = posts_corpus_basic.todense()\n",
    "#print(temp[0])\n",
    "posts_corpus_tokenized = []\n",
    "vocab_array = np.asarray(vocabulary).reshape(1, 653)\n",
    "for doc in posts_corpus_basic.todense():\n",
    "    one_doc = np.asarray(doc).astype(bool)\n",
    "    posts_corpus_tokenized.append(vocab_array[one_doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "99177c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "show_topics() got an unexpected keyword argument 'topics'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-182-bd5a48b79adf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mLDA_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel_lda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mposts_corpus_basic\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_topics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodeling_cores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m7\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mprint_topics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mLDA_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-181-b6ffadf7e45d>\u001B[0m in \u001B[0;36mprint_topics\u001B[1;34m(lda, vocab, n)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mprint_topics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlda\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvocab\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 39\u001B[1;33m     \u001B[0mtopics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlda\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow_topics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtopics\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtopn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mformatted\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     40\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mti\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtopic\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtopics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'topic %d: %s'\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mti\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m' '\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'%s/%.2f'\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtopic\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: show_topics() got an unexpected keyword argument 'topics'"
     ]
    }
   ],
   "source": [
    "LDA_model = model_lda(posts_corpus_basic, vocabulary, num_topics = 5, modeling_cores = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "34bf3d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.040*\"hold\" + 0.029*\"shares\" + 0.028*\"sell\" + 0.027*\"buy\" + 0.026*\"gme\" + 0.021*\"dont\" + 0.014*\"squeeze\" + 0.014*\"just\" + 0.014*\"holding\" + 0.013*\"short\"'),\n",
       " (1,\n",
       "  '0.091*\"gme\" + 0.076*\"amc\" + 0.061*\"buy\" + 0.049*\"bb\" + 0.041*\"lets\" + 0.039*\"nok\" + 0.038*\"moon\" + 0.022*\"robinhood\" + 0.019*\"just\" + 0.018*\"going\"'),\n",
       " (2,\n",
       "  '0.024*\"im\" + 0.016*\"just\" + 0.015*\"money\" + 0.014*\"like\" + 0.013*\"know\" + 0.011*\"people\" + 0.011*\"ive\" + 0.011*\"make\" + 0.010*\"dont\" + 0.010*\"going\"'),\n",
       " (3,\n",
       "  '0.012*\"company\" + 0.011*\"stock\" + 0.011*\"price\" + 0.010*\"market\" + 0.009*\"short\" + 0.008*\"shares\" + 0.007*\"going\" + 0.007*\"year\" + 0.007*\"time\" + 0.007*\"like\"'),\n",
       " (4,\n",
       "  '0.033*\"robinhood\" + 0.023*\"market\" + 0.022*\"hedge\" + 0.021*\"funds\" + 0.017*\"trading\" + 0.015*\"hedge funds\" + 0.012*\"stocks\" + 0.011*\"people\" + 0.011*\"manipulation\" + 0.011*\"like\"')]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print_topics(LDA_model, vocabulary, n = 10)\n",
    "LDA_model.show_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ab57610",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LDA_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-24-dc0685a8996c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgensim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdatapath\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mtemp_file\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatapath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"posts_model_1\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mLDA_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtemp_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mLDA_post_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mLDA_model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'LDA_model' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"posts_model_1\")\n",
    "LDA_model.save(temp_file)\n",
    "LDA_post_model = LDA_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e968f44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "##try to load the model already made to examine in pycharm\n",
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"posts_model_1\")\n",
    "LDA_post_model = LdaMulticore.load(temp_file)\n",
    "print(\"success\")\n",
    "#LDA_post_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40982e4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6911339380472198"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = corpora.Dictionary()\n",
    "d.id2token = LDA_post_model.id2word\n",
    "d.token2id = dict((k, v) for k, v in posts_cv.vocabulary_.items())\n",
    "post_dictionary = d\n",
    "##coherence model requires the original text - which has been lost due to count vectorizer\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=LDA_post_model, texts=posts_corpus_tokenized, dictionary = d)\n",
    "coherence_model_lda.get_coherence()\n",
    "#.69 coherence is pretty good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6919f45",
   "metadata": {},
   "source": [
    "LDA for Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeddb982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15k\n",
      "\n",
      "The produced Term/Document matrix vocab is of size: 20383\n",
      "0.04743504782624755% Non-Sparse\n"
     ]
    }
   ],
   "source": [
    "#comments_corpus_basic, comments_vocabulary = count_vectorize(comments_min, doc_frequency_threshhold = 5) # essentially all sparse\n",
    "#comments_corpus_basic, comments_vocabulary = count_vectorize(comments_min, doc_frequency_threshhold = 500)# 99.998% sparse, acceptable for now\n",
    "#print(\"5k\")\n",
    "#comments_corpus_basic, comments_vocabulary = count_vectorize(comments_min, doc_frequency_threshhold = 5000)#99.98% sparse, 1600 words\n",
    "print(\"15k\")\n",
    "comments_corpus_basic, comments_vocabulary, comments_cv = count_vectorize(comments_min, doc_frequency_threshhold = 300)#99.97% sparse, 600 words\n",
    "##will stick with 602 because comments will be at least as complex as posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abec3bef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#temp = comments_corpus_basic.todense()\n",
    "#print(temp[0])\n",
    "comments_corpus_tokenized = []\n",
    "vocab_array = np.asarray(comments_vocabulary).reshape(1, 20383)\n",
    "for doc in comments_corpus_basic:\n",
    "    doc = doc.todense()\n",
    "    one_doc = np.asarray(doc).astype(bool)\n",
    "    comments_corpus_tokenized.append(vocab_array[one_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5491eb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-f8b69b50242e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mLDA_comment_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel_lda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcomments_corpus_basic\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcomments_vocabulary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_topics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodeling_cores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m7\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-dcba461820af>\u001B[0m in \u001B[0;36mmodel_lda\u001B[1;34m(cvect_doc_fit, vocabulary, num_topics, passes, modeling_cores)\u001B[0m\n\u001B[0;32m     39\u001B[0m                         \u001B[1;33m,\u001B[0m \u001B[0mpasses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpasses\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m                         \u001B[1;33m,\u001B[0m \u001B[0mworkers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m7\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m                         , id2word = word_mapping)\n\u001B[0m\u001B[0;32m     42\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtemp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001B[0m\n\u001B[0;32m    182\u001B[0m             \u001B[0mdecay\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdecay\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moffset\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moffset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meval_every\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0meval_every\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0miterations\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0miterations\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m             \u001B[0mgamma_threshold\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgamma_threshold\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrandom_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mminimum_probability\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mminimum_probability\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 184\u001B[1;33m             \u001B[0mminimum_phi_value\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mminimum_phi_value\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mper_word_topics\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mper_word_topics\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    185\u001B[0m         )\n\u001B[0;32m    186\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001B[0m\n\u001B[0;32m    517\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcorpus\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    518\u001B[0m             \u001B[0muse_numpy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdispatcher\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 519\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mchunks_as_numpy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0muse_numpy\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    520\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    521\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0minit_dir_prior\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprior\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001B[0m in \u001B[0;36mupdate\u001B[1;34m(self, corpus, chunks_as_numpy)\u001B[0m\n\u001B[0;32m    303\u001B[0m                         \u001B[0mprocess_result_queue\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    304\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 305\u001B[1;33m                 \u001B[0mprocess_result_queue\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    306\u001B[0m             \u001B[1;31m# endfor single corpus pass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001B[0m in \u001B[0;36mprocess_result_queue\u001B[1;34m(force)\u001B[0m\n\u001B[0;32m    266\u001B[0m             \"\"\"\n\u001B[0;32m    267\u001B[0m             \u001B[0mmerged_new\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 268\u001B[1;33m             \u001B[1;32mwhile\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mresult_queue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    269\u001B[0m                 \u001B[0mother\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmerge\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult_queue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    270\u001B[0m                 \u001B[0mqueue_size\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m-=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\multiprocessing\\queues.py\u001B[0m in \u001B[0;36mempty\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 120\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_poll\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    121\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    122\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfull\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\multiprocessing\\connection.py\u001B[0m in \u001B[0;36mpoll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    255\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_closed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    256\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_readable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 257\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_poll\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    258\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__enter__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\multiprocessing\\connection.py\u001B[0m in \u001B[0;36m_poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    326\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0m_poll\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    327\u001B[0m             if (self._got_empty_message or\n\u001B[1;32m--> 328\u001B[1;33m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001B[0m\u001B[0;32m    329\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "LDA_comment_model = model_lda(comments_corpus_basic, comments_vocabulary, num_topics = 10, modeling_cores = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b40f1f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.047*\"im\" + 0.024*\"fucking\" + 0.023*\"holding\" + 0.017*\"calls\" + 0.014*\"pltr\" + 0.013*\"today\" + 0.012*\"gme\" + 0.012*\"bought\" + 0.010*\"moon\" + 0.010*\"lets\"'),\n",
       " (1,\n",
       "  '0.049*\"like\" + 0.020*\"just\" + 0.019*\"hands\" + 0.017*\"retarded\" + 0.016*\"days\" + 0.015*\"wait\" + 0.014*\"ago\" + 0.013*\"whats\" + 0.013*\"diamond\" + 0.012*\"look\"'),\n",
       " (2,\n",
       "  '0.026*\"make\" + 0.025*\"money\" + 0.024*\"good\" + 0.021*\"going\" + 0.016*\"im\" + 0.016*\"like\" + 0.011*\"sure\" + 0.008*\"year\" + 0.007*\"feel\" + 0.007*\"bad\"'),\n",
       " (3,\n",
       "  '0.019*\"stock\" + 0.013*\"short\" + 0.010*\"market\" + 0.010*\"long\" + 0.009*\"like\" + 0.008*\"just\" + 0.007*\"company\" + 0.007*\"price\" + 0.007*\"doesnt\" + 0.004*\"term\"'),\n",
       " (4,\n",
       "  '0.073*\"buy\" + 0.059*\"gme\" + 0.054*\"hold\" + 0.037*\"shares\" + 0.034*\"amc\" + 0.024*\"dip\" + 0.022*\"buying\" + 0.020*\"just\" + 0.018*\"bought\" + 0.017*\"bb\"'),\n",
       " (5,\n",
       "  '0.053*\"just\" + 0.049*\"dont\" + 0.031*\"know\" + 0.023*\"youre\" + 0.014*\"right\" + 0.012*\"want\" + 0.010*\"im\" + 0.009*\"like\" + 0.008*\"read\" + 0.008*\"advice\"'),\n",
       " (6,\n",
       "  '0.045*\"sell\" + 0.022*\"buy\" + 0.018*\"price\" + 0.017*\"open\" + 0.016*\"market\" + 0.015*\"calls\" + 0.013*\"selling\" + 0.013*\"puts\" + 0.012*\"close\" + 0.011*\"shares\"'),\n",
       " (7,\n",
       "  '0.021*\"shit\" + 0.018*\"fuck\" + 0.013*\"gonna\" + 0.011*\"day\" + 0.010*\"time\" + 0.009*\"guy\" + 0.009*\"fucking\" + 0.008*\"wsb\" + 0.008*\"lol\" + 0.007*\"stop\"'),\n",
       " (8,\n",
       "  '0.032*\"robinhood\" + 0.025*\"account\" + 0.020*\"rh\" + 0.019*\"trading\" + 0.018*\"ive\" + 0.014*\"im\" + 0.013*\"fidelity\" + 0.012*\"app\" + 0.011*\"does\" + 0.010*\"trade\"'),\n",
       " (9,\n",
       "  '0.034*\"people\" + 0.016*\"gme\" + 0.014*\"money\" + 0.011*\"think\" + 0.010*\"hedge\" + 0.010*\"thats\" + 0.010*\"lose\" + 0.010*\"funds\" + 0.009*\"theyre\" + 0.007*\"saying\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_comment_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9149b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = datapath(\"comment_model_1\")\n",
    "LDA_comment_model.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00bc56a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "temp_file = datapath(\"comment_model_1\")\n",
    "LDA_comment_model = LdaMulticore.load(temp_file)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19cd1435",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(0,\n  '0.137*\"buy\" + 0.087*\"shares\" + 0.056*\"hold\" + 0.041*\"gme\" + 0.037*\"robinhood\" + 0.035*\"bought\" + 0.034*\"dip\" + 0.032*\"sell\" + 0.031*\"just\" + 0.024*\"account\"'),\n (1,\n  '0.088*\"fuck\" + 0.063*\"thats\" + 0.060*\"shit\" + 0.034*\"let\" + 0.034*\"theyre\" + 0.030*\"yeah\" + 0.026*\"wont\" + 0.023*\"stop\" + 0.022*\"real\" + 0.022*\"time\"'),\n (2,\n  '0.144*\"dont\" + 0.077*\"know\" + 0.065*\"got\" + 0.051*\"need\" + 0.045*\"stock\" + 0.044*\"want\" + 0.043*\"like\" + 0.033*\"probably\" + 0.028*\"just\" + 0.024*\"guy\"'),\n (3,\n  '0.056*\"market\" + 0.045*\"calls\" + 0.042*\"price\" + 0.030*\"today\" + 0.024*\"day\" + 0.023*\"open\" + 0.019*\"shorts\" + 0.018*\"sell\" + 0.018*\"puts\" + 0.018*\"stock\"'),\n (4,\n  '0.070*\"good\" + 0.041*\"does\" + 0.037*\"say\" + 0.034*\"man\" + 0.032*\"better\" + 0.031*\"doesnt\" + 0.030*\"come\" + 0.029*\"work\" + 0.025*\"thanks\" + 0.025*\"way\"'),\n (5,\n  '0.222*\"im\" + 0.056*\"gonna\" + 0.046*\"think\" + 0.043*\"long\" + 0.039*\"just\" + 0.037*\"sure\" + 0.028*\"play\" + 0.026*\"ill\" + 0.023*\"looking\" + 0.022*\"pretty\"'),\n (6,\n  '0.134*\"gme\" + 0.072*\"amc\" + 0.059*\"holding\" + 0.049*\"moon\" + 0.043*\"bb\" + 0.040*\"squeeze\" + 0.032*\"hold\" + 0.032*\"short\" + 0.031*\"going\" + 0.031*\"week\"'),\n (7,\n  '0.131*\"money\" + 0.080*\"make\" + 0.048*\"lets\" + 0.031*\"lose\" + 0.030*\"youre\" + 0.029*\"yes\" + 0.027*\"pltr\" + 0.025*\"post\" + 0.024*\"gains\" + 0.022*\"free\"'),\n (8,\n  '0.041*\"people\" + 0.027*\"new\" + 0.025*\"like\" + 0.025*\"did\" + 0.023*\"ive\" + 0.021*\"actually\" + 0.020*\"didnt\" + 0.020*\"just\" + 0.019*\"year\" + 0.018*\"years\"'),\n (9,\n  '0.107*\"like\" + 0.078*\"right\" + 0.070*\"lol\" + 0.064*\"fucking\" + 0.032*\"retards\" + 0.031*\"retarded\" + 0.030*\"whats\" + 0.030*\"retard\" + 0.027*\"hes\" + 0.023*\"read\"')]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_comment_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c7dc42d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-16-445e49c750c1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mcoherence_comment_model_lda\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCoherenceModel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mLDA_comment_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtexts\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcomments_corpus_tokenized\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdictionary\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0md\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0mcoherence_comment_model_lda\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_coherence\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;31m#.54 coherence - could be better - but not terrible- improved to .58 with lemmatizing and stemming\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001B[0m in \u001B[0;36mget_coherence\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    607\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    608\u001B[0m         \"\"\"\n\u001B[1;32m--> 609\u001B[1;33m         \u001B[0mconfirmed_measures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_coherence_per_topic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    610\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maggregate_measures\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfirmed_measures\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    611\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001B[0m in \u001B[0;36mget_coherence_per_topic\u001B[1;34m(self, segmented_topics, with_std, with_support)\u001B[0m\n\u001B[0;32m    567\u001B[0m             \u001B[0msegmented_topics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmeasure\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseg\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtopics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    568\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_accumulator\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 569\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mestimate_probabilities\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msegmented_topics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    570\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    571\u001B[0m         \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwith_std\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mwith_std\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwith_support\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mwith_support\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001B[0m in \u001B[0;36mestimate_probabilities\u001B[1;34m(self, segmented_topics)\u001B[0m\n\u001B[0;32m    539\u001B[0m                 \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'model'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeyed_vectors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    540\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 541\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_accumulator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmeasure\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprob\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    542\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    543\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_accumulator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\topic_coherence\\probability_estimation.py\u001B[0m in \u001B[0;36mp_boolean_sliding_window\u001B[1;34m(texts, segmented_topics, dictionary, window_size, processes)\u001B[0m\n\u001B[0;32m    154\u001B[0m         \u001B[0maccumulator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mParallelWordOccurrenceAccumulator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocesses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtop_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdictionary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"using %s to estimate probabilities from sliding windows\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maccumulator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 156\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0maccumulator\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maccumulate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwindow_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    157\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py\u001B[0m in \u001B[0;36maccumulate\u001B[1;34m(self, texts, window_size)\u001B[0m\n\u001B[0;32m    442\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    443\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0maccumulate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtexts\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwindow_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 444\u001B[1;33m         \u001B[0mworkers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_q\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput_q\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart_workers\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwindow_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    445\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    446\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mqueue_all_texts\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_q\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtexts\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwindow_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py\u001B[0m in \u001B[0;36mstart_workers\u001B[1;34m(self, window_size)\u001B[0m\n\u001B[0;32m    476\u001B[0m             \u001B[0maccumulator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPatchedWordOccurrenceAccumulator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelevant_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdictionary\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    477\u001B[0m             \u001B[0mworker\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAccumulatingWorker\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_q\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput_q\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maccumulator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwindow_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 478\u001B[1;33m             \u001B[0mworker\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    479\u001B[0m             \u001B[0mworkers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mworker\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\multiprocessing\\process.py\u001B[0m in \u001B[0;36mstart\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    103\u001B[0m                \u001B[1;34m'daemonic processes are not allowed to have children'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    104\u001B[0m         \u001B[0m_cleanup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 105\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_popen\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_Popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    106\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sentinel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_popen\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msentinel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m         \u001B[1;31m# Avoid a refcycle if the target function holds an indirect\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\multiprocessing\\context.py\u001B[0m in \u001B[0;36m_Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    221\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_Popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 223\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_default_context\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mProcess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_Popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    224\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mDefaultContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseContext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\multiprocessing\\context.py\u001B[0m in \u001B[0;36m_Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    320\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0m_Popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    321\u001B[0m             \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mpopen_spawn_win32\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPopen\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 322\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mPopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    324\u001B[0m     \u001B[1;32mclass\u001B[0m \u001B[0mSpawnContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseContext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\multiprocessing\\popen_spawn_win32.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, process_obj)\u001B[0m\n\u001B[0;32m     63\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m                 \u001B[0mreduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprep_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mto_child\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m                 \u001B[0mreduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprocess_obj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mto_child\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m             \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m                 \u001B[0mset_spawning_popen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\multiprocessing\\reduction.py\u001B[0m in \u001B[0;36mdump\u001B[1;34m(obj, file, protocol)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     \u001B[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m     \u001B[0mForkingPickler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprotocol\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     61\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[1;31m#\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "d = corpora.Dictionary()\n",
    "d.id2token = LDA_comment_model.id2word\n",
    "d.token2id = dict((k, v) for k, v in comments_cv.vocabulary_.items())\n",
    "comment_dictionary = d\n",
    "##coherence model requires the original text - which has been lost due to count vectorizer\n",
    "\n",
    "coherence_comment_model_lda = CoherenceModel(model=LDA_comment_model, texts=comments_corpus_tokenized, dictionary = d)\n",
    "coherence_comment_model_lda.get_coherence()\n",
    "#.54 coherence - could be better - but not terrible- improved to .58 with lemmatizing and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = corpora.Dictionary()\n",
    "d.id2token = LDA_comment_model.id2word\n",
    "d.token2id = dict((k, v) for k, v in comments_cv.vocabulary_.items())\n",
    "comment_dictionary = d\n",
    "def try_lda_options(comments_corpus_basic, comments_vocabulary, comments_corpus_tokenized, d, num_topics = num_topics):\n",
    "    for topic_trial in num_topics:\n",
    "        LDA_comment_model = model_lda(comments_corpus_basic, comments_vocabulary, num_topics = topic_trial, modeling_cores = 7)\n",
    "        coherence_comment_model_lda = CoherenceModel(model=LDA_comment_model, texts=comments_corpus_tokenized, dictionary = d)\n",
    "        coherence_comment_model_lda.get_coherence()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90e1b2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([('gftdfnp',  't3_kcvkwx'),\n",
       "            ('gftdfk8',  't3_kcvkwx'),\n",
       "            ('gftdfh0', 't1_gftd073'),\n",
       "            ('gftdfek',  't3_kcvkwx'),\n",
       "            ('gftdfb7',  't3_kcvkwx'),\n",
       "            ('gftdf7b',  't3_kcvkwx'),\n",
       "            ('gftdf4z', 't1_gftcijz'),\n",
       "            ('gftdf1s',  't3_kcvkwx'),\n",
       "            ('gftdf0t',  't3_kcvkwx'),\n",
       "            ('gftdf0e',  't3_kcvkwx'),\n",
       "            ...\n",
       "            ('gkdmx3l', 't1_gkdh9q3'),\n",
       "            ('gkdmx31', 't1_gkcx6n1'),\n",
       "            ('gkdmx1g',  't3_l3aj4z'),\n",
       "            ('gkdmwu8', 't1_gkb9hsw'),\n",
       "            ('gkdmwoy',  't3_l35s4w'),\n",
       "            ('gkdmwie', 't1_gkdmaa1'),\n",
       "            ('gkdmw91', 't1_gkceomw'),\n",
       "            ('gkdmw6k',  't3_l3c0hj'),\n",
       "            ('gkdmvxa', 't1_gkbyl29'),\n",
       "            ('gkdmvqi', 't1_gkdgfwc')],\n",
       "           names=['id', 'parent_id'], length=5217259)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(comments_min.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47068384",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With both topic models created, final step is to print out the topics as well as create a mapping of topics to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a70614bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#posts_bow = [post_dictionary.doc2bow(text) for text in posts_corpus_tokenized]\n",
    "#topics_by_post = LDA_post_model[posts_bow]\n",
    "\n",
    "comments_bow = [comment_dictionary.doc2bow(text) for text in comments_corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "topics_by_comment = LDA_comment_model[comments_bow]\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5197 is out of bounds for axis 1 with size 520",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-41-f0a282c3c67d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtopics_by_comment\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    500\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mabc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbytes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    501\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mabc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSequence\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mExtensionArray\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 502\u001B[1;33m                 \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    503\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    504\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mis_dataclass\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\interfaces.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    179\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    180\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 181\u001B[1;33m                 \u001B[1;32myield\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdocno\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, bow, eps)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1523\u001B[0m         \"\"\"\n\u001B[1;32m-> 1524\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_document_topics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbow\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mminimum_phi_value\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mper_word_topics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1525\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1526\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'state'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'dispatcher'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mseparately\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36mget_document_topics\u001B[1;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001B[0m\n\u001B[0;32m   1327\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1328\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1329\u001B[1;33m         \u001B[0mgamma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mphis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minference\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbow\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcollect_sstats\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mper_word_topics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1330\u001B[0m         \u001B[0mtopic_dist\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgamma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgamma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# normalize distribution\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1331\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36minference\u001B[1;34m(self, chunk, collect_sstats)\u001B[0m\n\u001B[0;32m    678\u001B[0m             \u001B[0mElogthetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mElogtheta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    679\u001B[0m             \u001B[0mexpElogthetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexpElogtheta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 680\u001B[1;33m             \u001B[0mexpElogbetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpElogbeta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mids\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    681\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    682\u001B[0m             \u001B[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 5197 is out of bounds for axis 1 with size 520"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(topics_by_comment).head)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bcbc152",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#comment_topics = pd.DataFrame(topics_by_comment)\n",
    "#post_topics = pd.DataFrame(topics_by_post)\n",
    "\n",
    "comment_topics['id'] = comments_min.index\n",
    "#comment_topics['parent_id'] = comments_min['parent_id']\n",
    "\n",
    "#post_topics[id] = posts_min['id']\n",
    "\n",
    "comment_topics.to_csv(\"comments_with_topics.csv\")\n",
    "#post_topics.to_csv(\"posts_with_topics.csv\")\n",
    "\n",
    "#comment_topic_specs = LDA_comment_model\n",
    "\n",
    "pd.DataFrame(LDA_comment_model.show_topics()).to_csv(\"WSB_Comment_Topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eeefa26",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(LDA_comment_model.show_topics()).to_csv(\"WSB_Comment_Topics.csv\")\n",
    "#pd.DataFrame(LDA_post_model.show_topics()).to_csv(\"WSB_Post_Topics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb71b96",
   "metadata": {},
   "source": [
    "With the models created and saved, lets see how meaningful they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "761d690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d7d4a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x0000012EB1EEFC50>\n"
     ]
    }
   ],
   "source": [
    "#This nice function above requires the gensim bag of words representation. We'll give it just that'\n",
    "comments_small = pd.DataFrame(comments[:10])\n",
    "corpus_small = comments_corpus_tokenized[:10]\n",
    "##doc to bag of words ONLY WORKS WITH AN ARRAY OF WORDS\n",
    "corpus_bow_small = [comment_dictionary.doc2bow(text) for text in corpus_small]\n",
    "print(LDA_comment_model[corpus_bow_small])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5197 is out of bounds for axis 1 with size 520",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-37-5ee24774aabf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf_topic_sents_keywords\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mformat_topics_sentences\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mldamodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mLDA_comment_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcomments_bow\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtexts\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcomments_corpus_tokenized\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# Format\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mdf_dominant_topic\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_topic_sents_keywords\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mdf_dominant_topic\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'Document_No'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Dominant_Topic'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Topic_Perc_Contrib'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Keywords'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Text'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-19-d5f063bb4355>\u001B[0m in \u001B[0;36mformat_topics_sentences\u001B[1;34m(ldamodel, corpus, texts)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;31m# Get main topic in each document\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrow\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mldamodel\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m         \u001B[0mrow\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreverse\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[1;31m# Get the Dominant topic, Perc Contribution and Keywords for each document\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\interfaces.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    179\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    180\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 181\u001B[1;33m                 \u001B[1;32myield\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdocno\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, bow, eps)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1523\u001B[0m         \"\"\"\n\u001B[1;32m-> 1524\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_document_topics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbow\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mminimum_phi_value\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mper_word_topics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1525\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1526\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'state'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'dispatcher'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mseparately\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36mget_document_topics\u001B[1;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001B[0m\n\u001B[0;32m   1327\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1328\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1329\u001B[1;33m         \u001B[0mgamma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mphis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minference\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbow\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcollect_sstats\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mper_word_topics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1330\u001B[0m         \u001B[0mtopic_dist\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgamma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgamma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# normalize distribution\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1331\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36minference\u001B[1;34m(self, chunk, collect_sstats)\u001B[0m\n\u001B[0;32m    678\u001B[0m             \u001B[0mElogthetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mElogtheta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    679\u001B[0m             \u001B[0mexpElogthetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexpElogtheta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 680\u001B[1;33m             \u001B[0mexpElogbetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpElogbeta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mids\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    681\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    682\u001B[0m             \u001B[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 5197 is out of bounds for axis 1 with size 520"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=LDA_comment_model, corpus=comments_bow, texts=comments_corpus_tokenized)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5197 is out of bounds for axis 1 with size 520",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-38-86fa6afb1d13>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrow\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mLDA_comment_model\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcomments_bow\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\interfaces.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    179\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    180\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 181\u001B[1;33m                 \u001B[1;32myield\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdocno\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, bow, eps)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1523\u001B[0m         \"\"\"\n\u001B[1;32m-> 1524\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_document_topics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbow\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mminimum_phi_value\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mper_word_topics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1525\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1526\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'state'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'dispatcher'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mseparately\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36mget_document_topics\u001B[1;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001B[0m\n\u001B[0;32m   1327\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1328\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1329\u001B[1;33m         \u001B[0mgamma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mphis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minference\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbow\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcollect_sstats\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mper_word_topics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1330\u001B[0m         \u001B[0mtopic_dist\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgamma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgamma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# normalize distribution\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1331\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\BigDataIST718\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001B[0m in \u001B[0;36minference\u001B[1;34m(self, chunk, collect_sstats)\u001B[0m\n\u001B[0;32m    678\u001B[0m             \u001B[0mElogthetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mElogtheta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    679\u001B[0m             \u001B[0mexpElogthetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexpElogtheta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 680\u001B[1;33m             \u001B[0mexpElogbetad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpElogbeta\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mids\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    681\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    682\u001B[0m             \u001B[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 5197 is out of bounds for axis 1 with size 520"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(LDA_comment_model[comments_bow]):\n",
    "    print(i)\n",
    "    print(row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}